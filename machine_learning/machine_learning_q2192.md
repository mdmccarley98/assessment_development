#


## Item ID
2192

## Claim

4

## Claim Behavior (evidence)

[SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)

[SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/pdf/1608.03983v5.pdf)

## Content Target

Machine Learning Models

## Cognitive Model

Comprehend

## Item Type

Multiple Choice

## Stem

When using stochastic gradient descent, which of the following would NOT be an appropriate learning rate schedule? 

## Code Snippet (optional)


## Answer Key

Linearly increase the learning rate with each iteration

## Distractors
### 1.

Reduce learning rate to half whenever the loss stops decreasing

### 2.

Reduce learning rate following an exponential decay curve

### 3.

Gradually decrease learning rate, but periodically increase it back to the initial rate


## Common errors, misconceptions, or irrelevant information:

A too-large learning rate will cause the model to diverge. Distractors 1 and 2 are classic schedules, whereas Distractor 3 is a more recent (2017) development.


# Triplebyte Review


## Language Review: (TB only)


## Bias and Fairness Review: (TB only)


## Content Review: (TB only)

