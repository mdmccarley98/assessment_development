#


## Item ID
2201

## Claim

4

## Claim Behavior (evidence)

[Hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)

## Content Target

Machine Learning Models

## Cognitive Model

Comprehend

## Item Type

Multiple Choice

## Stem

Consider a machine learning model with multiple tunable hyperparameters. Which of the following is NOT a generally valid strategy for finding the best combination of hyperparameters?

## Code Snippet (optional)


## Answer Key

Gradient descent

## Distractors
### 1.

Evolutionary methods

### 2.

Bayesian optimization

### 3.

Random search


## Common errors, misconceptions, or irrelevant information:

Gradient descent would require you to compute the gradient of the final model's validation error with respect to the hyperparameters, which is not possible in the general case.

The other three methods work in a "black box" fashion.

# Triplebyte Review


## Language Review: (TB only)


## Bias and Fairness Review: (TB only)


## Content Review: (TB only)

