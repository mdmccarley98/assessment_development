#


## Item ID
2215

## Claim

1

## Claim Behavior (evidence)



> From either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix.

>  The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data.

-- [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)


> The input data is centered but not scaled for each feature before applying the SVD.

-- [PCA, scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.transform)


## Content Target

Data Manipulation

## Cognitive Model

Comprehend

## Item Type

Multiple Choice

## Stem

Principal component analysis (PCA) is a common method for dimensionality reduction. 

Consider a `n x d` matrix `X` with `n` observations and `d` features.

After using PCA to reduce `X` to `d/2` output columns, which of the following is necessarily true?


## Code Snippet (optional)

## Answer Key

The mean of each output column will be zero

## Distractors
### 1.

The first principal component is a direction that minimizes the variance of the projected data

### 2.

Applying the inverse transform to the output recovers the original `X`

### 3.

The principal components are eigenvectors of the matrix `X`


## Common errors, misconceptions, or irrelevant information:


# Triplebyte Review


## Language Review: (TB only)


## Bias and Fairness Review: (TB only)


## Content Review: (TB only)

