#


## Item ID
2208

## Claim

4

## Claim Behavior (evidence)


> Compared to SGD, L-BFGS is more computationally expensive and requires more memory, because it needs to approximate the Hessian matrix and stores more search information. On the other hand, L-BFGS is usually more stable, and has fewer hyperparameters to tune.
> 
- [Optimization Algorithms](https://documentation.sas.com/doc/en/casdlpg/8.3/p1i430hb7ri1fnn1511qu227wtbw.htm#p1thtxa0porjkan1d58of2iqx4k3)

> A strength of SGDs is that they are simple to implement and also fast for problems that have many training examples. 

> Batch methods, such as L-BFGS (...) also enjoy parallelism by computing the gradient on GPUs

- [On Optimization Methods for Deep Learning](https://icml.cc/2011/papers/210_icmlpaper.pdf)

## Content Target

Machine Learning Models

## Cognitive Model

Comprehend

## Item Type

Multiple Choice

## Stem

Stochastic gradient descent (SGD) and Limited-memory BFGS (L-BFGS) are two common approaches for minimizing a differentiable objective function. 

SGD uses only the gradient of the objective, while L-BFGS estimates the second derivative as well. Which of the following is a valid comparison of the two algorithms?

## Code Snippet (optional)


## Answer Key

L-BFGS requires more memory than SGD

## Distractors
### 1.

L-BFGS is simpler to implement than SGD

### 2.

L-BFGS cannot be implemented on GPU but SGD can be

### 3.

L-BFGS has no tunable hyperparameters but SGD does


## Common errors, misconceptions, or irrelevant information:




# Triplebyte Review


## Language Review: (TB only)


## Bias and Fairness Review: (TB only)


## Content Review: (TB only)

